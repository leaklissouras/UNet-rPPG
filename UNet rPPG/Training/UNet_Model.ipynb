{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f41ba0-f218-4efd-9e16-f985234c15e4",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "U-Net Model Training. Model takes MST Maps as input and produces predicted vPPG or PPG signals. Model adapted from Yu et al (see references)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618789a0-238e-4a59-84c7-c02cd89997a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "import visualkeras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bc0e1-2d8b-4959-b9ee-71654fa7fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subjects used for training were placed in a seperate folder; use base path to define location of training subject videos\n",
    "\n",
    "base_path = os.getcwd() #change path based on location of training folder\n",
    "videos = glob.glob(os.path.join(base_path,\"subject*/vid.avi\"))\n",
    "subjects_train =  []\n",
    "\n",
    "for i in videos:\n",
    "    subject_number = i[-10:-8]\n",
    "    try:\n",
    "        subject_number = subject_number.replace('t','')\n",
    "    except:\n",
    "        pass\n",
    "    subjects_train.append(f\"subject{subject_number}\")\n",
    "\n",
    "print(subjects_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fd6bf-6e50-4852-a8fa-30dad39aec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for subject in subjects_train:\n",
    "    iterate = len(glob.glob(f\"{base_path}/{subject}/Map*\")) #change based on save location of MST Maps\n",
    "    for i in range(1,iterate+1):\n",
    "        img_path_rgb = os.path.join(base_path, subject, f'Map{i}', 'img_rgb.png')\n",
    "        img_path_yuv = os.path.join(base_path, subject, f'Map{i}', 'img_yuv.png')\n",
    "        \n",
    "        img_rgb = load_img(img_path_rgb,color_mode = 'rgb')\n",
    "        img_yuv = load_img(img_path_yuv, color_mode = 'rgb')\n",
    "        combined_image = np.concatenate((img_rgb, img_yuv),axis = -1)\n",
    "        \n",
    "        X_train.append(combined_image)\n",
    "        \n",
    "        bvp_path = os.path.join(base_path,subject, f'Map{i}', 'bvp.mat')\n",
    "        bvp_data = loadmat(bvp_path)\n",
    "        bvp =  bvp_data['bvp'][0]\n",
    "        \n",
    "        y_train.append(bvp)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162fa6df-f817-4a0c-92ff-13987f5d24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(input, num_filters):\n",
    "    kernel_size = (3,3)\n",
    "    stride = 1\n",
    "    x = layers.Conv2D(filters=num_filters, kernel_size=(3,3), strides=1, padding='same')(input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.PReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters=num_filters, kernel_size=(3,3), strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.PReLU()(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f224ad-a628-4aff-bbc6-0ee51bd1419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = layers.Input(shape=(63,256,6))\n",
    "\n",
    "x0 = layers.ZeroPadding2D(padding=((0,1),(0,0)))(input_tensor)\n",
    "x1 = double_conv_block(x0,32)\n",
    "\n",
    "x2 = layers.AveragePooling2D((2,2))(x1)\n",
    "x2 = double_conv_block(x2,64)\n",
    "\n",
    "x3 = layers.AveragePooling2D((2,2))(x2)\n",
    "x3 = double_conv_block(x3,128)\n",
    "\n",
    "x4 = layers.AveragePooling2D((2,2))(x3)\n",
    "x4 = double_conv_block(x4,256)\n",
    "\n",
    "x5 = layers.AveragePooling2D((2,2))(x4)\n",
    "x5 = double_conv_block(x5,512)\n",
    "\n",
    "x6 = layers.Conv2DTranspose(filters = int(512/2),strides = (2,2),kernel_size = (1,1),padding = 'same')(x5)\n",
    "\n",
    "x7 = layers.Concatenate(axis=-1)([x4,x6])   #Concatenate along the channel dimension\n",
    "x7 = double_conv_block(x7,256)\n",
    "x7 = layers.Conv2DTranspose(filters = int(256/2),strides = (2,2),kernel_size = (1,1),padding = 'same')(x7)\n",
    "\n",
    "x8 = layers.Concatenate(axis=-1)([x3,x7])\n",
    "x8 = double_conv_block(x8,128)\n",
    "x8 = layers.Conv2DTranspose(filters = int(128/2),strides = (2,2),kernel_size = (1,1),padding = 'same')(x8)\n",
    "\n",
    "x9 = layers.Concatenate(axis=-1)([x2,x8])\n",
    "x9 = double_conv_block(x8,64)\n",
    "x9 = layers.Conv2DTranspose(filters = int(64/2),strides = (2,2),kernel_size = (1,1),padding = 'same')(x9)\n",
    "\n",
    "x10 = layers.Concatenate(axis=-1)([x1,x9])        \n",
    "x10 = double_conv_block(x9,32)\n",
    "\n",
    "x11 = layers.AveragePooling2D(pool_size = (64,1), strides=(64,1),padding = 'valid')(x10) #Global Average Pooling Layer\n",
    "x11 = layers.Conv2D(filters=1, kernel_size=(1,1), strides=1, padding='same')(x11)\n",
    "\n",
    "x11 = layers.Flatten()(x11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f92d40-73bb-4a8e-adda-44bb0ae6f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=input_tensor, outputs=x11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273a5c0-730b-4488-b782-8dc977a6938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fba43b-8892-442b-a79d-0a9e1510ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig(i, history):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1,epochs+1),history.history['val_accuracy'],label='validation')\n",
    "    plt.plot(range(1,epochs+1),history.history['accuracy'],label='training')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlim([1,epochs])\n",
    "#   plt.ylim([0,1])\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220880d-3088-4953-ad1f-1efc7eb14dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_pearson_loss(y_true,y_pred): \n",
    "    y_true = tf.reshape(y_true,[-1])\n",
    "    y_pred = tf.reshape(y_pred,[-1])\n",
    "\n",
    "    mean_y_true = tf.reduce_mean(y_true)\n",
    "    mean_y_pred = tf.reduce_mean(y_pred)\n",
    "\n",
    "    y_true_cent = y_true - mean_y_true\n",
    "    y_pred_cent = y_pred - mean_y_pred\n",
    "\n",
    "    numerator = tf.reduce_sum(y_true_cent * y_pred_cent)\n",
    "\n",
    "    denominator = tf.sqrt(tf.reduce_sum(y_true_cent ** 2) * tf.reduce_sum(y_pred_cent ** 2))\n",
    "\n",
    "    pearson_corr = numerator/(denominator + 1e-8) #prevents division by zero\n",
    "\n",
    "    loss = 1 - pearson_corr\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d924a-c069-4972-8366-f4c9062e0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_and_mse_loss(y_true, y_pred):\n",
    "    # Perform Fourier Transform on the predictions (keep complex64)\n",
    "    y_pred_freq = tf.signal.fft(tf.cast(y_pred, tf.complex64))\n",
    "    y_pred_power = tf.abs(y_pred_freq)**2  # Power spectrum of predicted\n",
    "\n",
    "    # Perform Fourier Transform on the true values (keep complex64)\n",
    "    y_true_freq = tf.signal.fft(tf.cast(y_true, tf.complex64))\n",
    "    y_true_power = tf.abs(y_true_freq)**2  # Power spectrum of true values\n",
    "\n",
    "    # Extract peak indices from the power spectra (excluding DC component)\n",
    "    # Apply argmax on the magnitude (real part) of the power spectrum\n",
    "    peak_index_pred = tf.argmax(tf.abs(y_pred_power[1:]), axis=-1) + 1  # Excludes DC component\n",
    "    peak_index_true = tf.argmax(tf.abs(y_true_power[1:]), axis=-1) + 1  # Excludes DC component\n",
    "\n",
    "    # Compute predicted heart rate and true heart rate (bpm)\n",
    "    predicted_hr = tf.cast(peak_index_pred, tf.float32) * 60  # Multiply by 60 for bpm\n",
    "    true_hr = tf.cast(peak_index_true, tf.float32) * 60  # Multiply by 60 for bpm\n",
    "\n",
    "    # Calculate Manhattan loss\n",
    "    manhattan_loss = tf.abs(predicted_hr - true_hr)\n",
    "\n",
    "    # Calculate MSE loss based on power spectra\n",
    "    mse_loss = tf.reduce_mean(tf.square(y_pred_power - y_true_power))\n",
    "\n",
    "    # Combine losses with specified weights\n",
    "    loss = 0.2 * manhattan_loss + 0.1 * mse_loss\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4cba8-b848-4982-af02-37b764abd681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_loss(y_true,y_pred):\n",
    "    loss1 = negative_pearson_loss(y_true,y_pred) #negative pearson loss (coefficient = 1)\n",
    "    loss2 = manhattan_and_mse_loss(y_true,y_pred) #Manhattan and MSE loss (coefficients already incorporated)\n",
    "\n",
    "    tot_loss = loss1 + loss2\n",
    "\n",
    "    return tot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76deb60-6486-455d-bd94-c78efe15e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "epochs = 50\n",
    "\n",
    "model.compile(optimizer=optimizer,loss=combined_loss, metrics = ['root_mean_squared_error','mean_squared_error'])\n",
    "\n",
    "history1 = model.fit(X_train,y_train,\n",
    "                      batch_size = 64,\n",
    "                      epochs = 50,\n",
    "                      verbose = 1)\n",
    "\n",
    "\n",
    "model.save('trained_rPPG_U-Net.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e57d9-204c-4214-ad2b-ebc6899d49fe",
   "metadata": {},
   "source": [
    "### Optional Graphs for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc3b01-aa17-4215-bf29-a5534c5c1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if 'val_loss' is available in history to plot\n",
    "if 'val_loss' in history1.history:\n",
    "    plt.plot(history1.history['loss'], label='Training Loss')\n",
    "    plt.plot(history1.history['val_loss'], label='Validation Loss')\n",
    "else:\n",
    "    plt.plot(history1.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.title('Model Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9025f-4360-4ba9-ae6e-644bea086447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig(i, history):\n",
    "    fig = plt.figure()\n",
    "    # Plot root mean squared error for training\n",
    "    plt.plot(range(1, epochs + 1), history.history['root_mean_squared_error'], label='Training RMSE')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Root Mean Squared Error')\n",
    "    plt.xlim([1, epochs])\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training RMSE Over Epochs\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# After training, use this function to plot the results\n",
    "plot_fig(1, history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eed91e-fde7-4d19-a8a2-093c261d0b76",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Yu SN, Wang CS, Yu Ping Chang. Heart Rate Estimation From Remote Photoplethysmography Based on Light-Weight U-Net and Attention Modules.  IEEE access. 2023;11:54058-54069. doi:https://doi.org/10.1109/access.2023.3281898 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
